{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0814df2",
   "metadata": {},
   "source": [
    "\n",
    "### üåü **Non-Linear Supervised Learning Algorithms ‚Äì Short Notes**\n",
    "\n",
    "**Definition**:\n",
    "Supervised learning algorithms learn from labeled data to predict outcomes. **Non-linear algorithms** can model complex relationships where the data cannot be separated or fitted using a straight line.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå 1. **Decision Trees**\n",
    "\n",
    "* **Type**: Classification & Regression\n",
    "* **Working**: Splits data based on feature values into branches; non-linear boundaries.\n",
    "* **Pros**: Easy to interpret; handles both numerical & categorical data.\n",
    "* **Cons**: Prone to overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå 2. **Random Forest**\n",
    "\n",
    "* **Type**: Ensemble (Classification & Regression)\n",
    "* **Working**: Builds multiple decision trees and combines their outputs.\n",
    "* **Pros**: Reduces overfitting; good accuracy.\n",
    "* **Cons**: Less interpretable; slower than single trees.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå 3. **Support Vector Machines (SVM) with Non-Linear Kernels**\n",
    "\n",
    "* **Type**: Classification & Regression\n",
    "* **Working**: Uses kernels (like RBF, polynomial) to map data into higher dimensions to find a separating hyperplane.\n",
    "* **Pros**: Effective in high dimensions.\n",
    "* **Cons**: Computationally expensive with large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå 4. **K-Nearest Neighbors (KNN)**\n",
    "\n",
    "* **Type**: Classification & Regression\n",
    "* **Working**: Classifies based on majority vote of nearest data points (non-linear decision boundaries).\n",
    "* **Pros**: Simple; no training phase.\n",
    "* **Cons**: Slow with large data; sensitive to feature scaling.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Algorithm         | Use Case | Strength                     | Weakness                |\n",
    "| ----------------- | -------- | ---------------------------- | ----------------------- |\n",
    "| Decision Tree     | C & R    | Interpretability             | Overfitting             |\n",
    "| Random Forest     | C & R    | Accuracy, robustness         | Complexity              |\n",
    "| SVM (Non-linear)  | C & R    | Effective in high dim spaces | High computational cost |\n",
    "| KNN               | C & R    | Simple, no training needed   | Slow prediction         |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f41f7e3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üìö **K-Nearest Neighbors (KNN) ‚Äì Complete Notes**\n",
    "\n",
    "---\n",
    "\n",
    "### üî∑ **What is KNN?**\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a **supervised learning algorithm** used for both **classification** and **regression**. It is a **lazy learner** because it doesn‚Äôt build a model during training but makes predictions only at runtime.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **How KNN Works**\n",
    "\n",
    "1. **Choose K** (number of neighbors).\n",
    "2. **Calculate distance** between the new point and all training points.\n",
    "3. **Sort the distances** and find the **K nearest neighbors**.\n",
    "4. For **classification**:\n",
    "\n",
    "   * Use **majority vote** of K neighbors‚Äô classes.\n",
    "5. For **regression**:\n",
    "\n",
    "   * Take the **average (or weighted average)** of K neighbors‚Äô values.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **KNN for Classification ‚Äì Example**\n",
    "\n",
    "#### üßæ Dataset (Fruit Classification)\n",
    "\n",
    "| Fruit  | Weight (g) | Color Score | Label  |\n",
    "| ------ | ---------- | ----------- | ------ |\n",
    "| Apple  | 150        | 0.8         | Apple  |\n",
    "| Apple  | 170        | 0.75        | Apple  |\n",
    "| Orange | 130        | 0.4         | Orange |\n",
    "| Orange | 120        | 0.35        | Orange |\n",
    "| Banana | 110        | 0.9         | Banana |\n",
    "\n",
    "#### üÜï New Fruit to Classify\n",
    "\n",
    "* Weight: **140 g**\n",
    "* Color Score: **0.6**\n",
    "\n",
    "#### ‚úÖ Python Code (Classification)\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Features and labels\n",
    "X = [[150, 0.8], [170, 0.75], [130, 0.4], [120, 0.35], [110, 0.9]]\n",
    "y = ['Apple', 'Apple', 'Orange', 'Orange', 'Banana']\n",
    "\n",
    "# New data\n",
    "new_data = [[140, 0.6]]\n",
    "\n",
    "# KNN Classifier\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(new_data)\n",
    "print(\"Predicted Class:\", prediction[0])\n",
    "```\n",
    "\n",
    "#### üéØ Output\n",
    "\n",
    "```\n",
    "Predicted Class: Apple\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **KNN for Regression ‚Äì Example**\n",
    "\n",
    "#### üßæ Dataset (House Price Prediction)\n",
    "\n",
    "| House Size (sqft) | Price (in \\$1000) |\n",
    "| ----------------- | ----------------- |\n",
    "| 1000              | 300               |\n",
    "| 1200              | 330               |\n",
    "| 1500              | 380               |\n",
    "| 1700              | 410               |\n",
    "| 2000              | 450               |\n",
    "\n",
    "#### üÜï Predict Price of House\n",
    "\n",
    "* Size: **1600 sqft**\n",
    "\n",
    "#### ‚úÖ Python Code (Regression)\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Features and target\n",
    "X = [[1000], [1200], [1500], [1700], [2000]]\n",
    "y = [300, 330, 380, 410, 450]\n",
    "\n",
    "# New data\n",
    "new_house = [[1600]]\n",
    "\n",
    "# KNN Regressor\n",
    "model = KNeighborsRegressor(n_neighbors=3)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(new_house)\n",
    "print(\"Predicted Price: $\", prediction[0]*1000)\n",
    "```\n",
    "\n",
    "#### üéØ Output\n",
    "\n",
    "```\n",
    "Predicted Price: $400000.0\n",
    "```\n",
    "\n",
    "> Here, the average price of the 3 closest house sizes (1500, 1700, 2000) is used for prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß **Choosing the Right K**\n",
    "\n",
    "* **Too small (e.g., K=1)** ‚Üí Overfitting (model too sensitive to noise).\n",
    "* **Too large** ‚Üí Underfitting (model too smooth).\n",
    "* Use **cross-validation** to find the optimal K.\n",
    "\n",
    "---\n",
    "\n",
    "### üìê **Distance Metrics**\n",
    "\n",
    "* **Euclidean Distance** (default for most cases):\n",
    "\n",
    "  $$\n",
    "  d = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\n",
    "  $$\n",
    "* **Other options**: Manhattan, Minkowski, cosine distance.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Advantages**\n",
    "\n",
    "* Simple to understand and implement.\n",
    "* Works well with small, clean datasets.\n",
    "* No training time ‚Äì just store data.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå **Disadvantages**\n",
    "\n",
    "* Slow prediction on large datasets.\n",
    "* Memory-intensive (stores all data).\n",
    "* Performance drops in **high dimensions** (curse of dimensionality).\n",
    "* Sensitive to feature scaling (important to normalize or standardize features).\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ **Applications of KNN**\n",
    "\n",
    "* **Image recognition**\n",
    "* **Handwriting classification**\n",
    "* **Medical diagnosis**\n",
    "* **Recommendation systems**\n",
    "* **Customer segmentation**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad51f3c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üå≥ Decision Tree ‚Äì Detailed Explanation\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **What is a Decision Tree?**\n",
    "\n",
    "* A **Decision Tree** is a **tree-shaped model** used for supervised learning.\n",
    "* It predicts **target values** by learning simple decision rules inferred from data features.\n",
    "* Applicable for **classification** (discrete labels) and **regression** (continuous values).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Structure of a Decision Tree**\n",
    "\n",
    "* **Root Node:** The top node representing the entire dataset.\n",
    "* **Internal Nodes (Decision Nodes):** Nodes where data is split based on a feature condition.\n",
    "* **Branches:** Paths from one node to another depending on split decisions.\n",
    "* **Leaf Nodes (Terminal Nodes):** Final nodes that give the output (class label or regression value).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **How Does a Decision Tree Work?**\n",
    "\n",
    "* **Step 1:** Start at the root with the full dataset.\n",
    "* **Step 2:** Evaluate all possible splits across all features.\n",
    "* **Step 3:** Choose the split that best separates the data according to a **splitting criterion**.\n",
    "* **Step 4:** Split the dataset into subsets based on the selected feature and threshold.\n",
    "* **Step 5:** Recursively repeat Steps 2‚Äì4 on each subset.\n",
    "* **Step 6:** Stop splitting when a stopping condition is met:\n",
    "\n",
    "  * All samples in a node belong to the same class (classification).\n",
    "  * Minimum samples per node reached.\n",
    "  * Maximum tree depth reached.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Splitting Criteria (How to Choose Best Split)**\n",
    "\n",
    "### For Classification:\n",
    "\n",
    "* **Gini Impurity:**\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "$$\n",
    "\n",
    "Where $p_i$ is the proportion of class $i$ in the node. Lower Gini means better purity.\n",
    "\n",
    "* **Entropy (Information Gain):**\n",
    "\n",
    "$$\n",
    "Entropy = - \\sum_{i=1}^C p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "Information gain = Entropy(parent) - Weighted average Entropy(children)\n",
    "\n",
    "Higher information gain means better split.\n",
    "\n",
    "### For Regression:\n",
    "\n",
    "* **Mean Squared Error (MSE):**\n",
    "\n",
    "Split chosen to minimize the variance of target values in child nodes.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Stopping Conditions**\n",
    "\n",
    "* **Pure node:** All examples belong to the same class.\n",
    "* **Max depth:** Limit the depth of the tree to avoid overfitting.\n",
    "* **Minimum samples per split/leaf:** Prevent splits with too few samples.\n",
    "* **No improvement:** No further decrease in impurity.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **How to Use Decision Trees?**\n",
    "\n",
    "### Classification Example:\n",
    "\n",
    "| Feature: Age | Buys Laptop (Label) |\n",
    "| ------------ | ------------------- |\n",
    "| < 30         | No                  |\n",
    "| 30‚Äì40        | Yes                 |\n",
    "| > 40         | Yes                 |\n",
    "\n",
    "* The tree first splits by Age < 30?\n",
    "* If yes, predict **No**.\n",
    "* Else, predict **Yes**.\n",
    "\n",
    "### Regression Example:\n",
    "\n",
    "| House Size (sqft) | Price (in \\$1000) |\n",
    "| ----------------- | ----------------- |\n",
    "| 1000              | 200               |\n",
    "| 1500              | 300               |\n",
    "| 2000              | 400               |\n",
    "\n",
    "* Splits data based on size ranges minimizing variance in prices within each leaf.\n",
    "* Predicts the average price in the leaf node.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Advantages**\n",
    "\n",
    "* **Easy to understand and interpret** (decision paths are human-readable).\n",
    "* Handles **both categorical and numerical data**.\n",
    "* **No need for feature scaling or normalization**.\n",
    "* Can model **non-linear relationships**.\n",
    "* Fast predictions once trained.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. **Disadvantages**\n",
    "\n",
    "* Can easily **overfit** especially if tree is very deep.\n",
    "* Small changes in data can lead to **very different trees** (unstable).\n",
    "* Decision boundaries are **axis-aligned**, which may limit model flexibility.\n",
    "* Not great with **high-dimensional data** without feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. **Avoiding Overfitting**\n",
    "\n",
    "* **Pruning:** Cut back parts of the tree that do not improve performance on validation data.\n",
    "* **Set max depth:** Limit how deep the tree can grow.\n",
    "* **Minimum samples leaf:** Set a minimum number of samples needed in a leaf node.\n",
    "* Use **ensemble methods** like Random Forest or Gradient Boosted Trees for better generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. **Hyperparameters to Tune**\n",
    "\n",
    "| Parameter           | Purpose                                    |\n",
    "| ------------------- | ------------------------------------------ |\n",
    "| `max_depth`         | Maximum levels of the tree                 |\n",
    "| `min_samples_split` | Minimum samples required to split a node   |\n",
    "| `min_samples_leaf`  | Minimum samples required in a leaf node    |\n",
    "| `criterion`         | Split quality measure (gini, entropy, mse) |\n",
    "| `max_features`      | Max features to consider for split         |\n",
    "\n",
    "---\n",
    "\n",
    "## 11. **Python Code Example (Classification)**\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = [[25], [35], [45], [20], [50]]\n",
    "y = ['No', 'Yes', 'Yes', 'No', 'Yes']\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3, criterion='gini')\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.predict([[28]]))  # Output: 'No' or 'Yes'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 12. **Visualizing Decision Trees**\n",
    "\n",
    "* Trees can be visualized with `plot_tree` in sklearn or `graphviz`.\n",
    "* Visualization helps understand how the tree makes decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Aspect           | Details                                                       |\n",
    "| ---------------- | ------------------------------------------------------------- |\n",
    "| Type             | Supervised, non-linear                                        |\n",
    "| Suitable for     | Classification & regression                                   |\n",
    "| Interpretability | High (easy to interpret)                                      |\n",
    "| Pros             | Simple, no scaling needed, handles categorical/numerical data |\n",
    "| Cons             | Prone to overfitting, unstable, axis-aligned splits           |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eed5cb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Support Vector Machine (SVM) ‚Äî Clear & Simple Guide\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **What is SVM?**\n",
    "\n",
    "* SVM is a **supervised learning algorithm** used for:\n",
    "\n",
    "  * **Classification** (e.g., spam or not spam)\n",
    "  * **Regression** (predicting continuous values, less common)\n",
    "\n",
    "* Goal: Find the **best boundary** (called a **hyperplane**) that **separates different classes** in the data.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **What is a Hyperplane?**\n",
    "\n",
    "* In 2D, a hyperplane is a **line** that divides the plane.\n",
    "* In 3D, it‚Äôs a **plane** dividing the space.\n",
    "* In higher dimensions, it‚Äôs a flat decision boundary.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **How does SVM find the best hyperplane?**\n",
    "\n",
    "* SVM looks for the hyperplane that **maximizes the margin**:\n",
    "\n",
    "  * The margin = distance between the hyperplane and the **closest points** of each class.\n",
    "  * These closest points are called **support vectors**.\n",
    "* A bigger margin means better generalization to unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Why margin matters?**\n",
    "\n",
    "* A **wide margin** means the decision boundary is far from any data point, making the model more robust to noise.\n",
    "* A **narrow margin** means the boundary is close to some data points and more likely to misclassify new points.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **What if data is not linearly separable?**\n",
    "\n",
    "* Many real-world problems have data that **can‚Äôt be separated by a straight line**.\n",
    "* SVM uses something called the **Kernel Trick**:\n",
    "\n",
    "  * It transforms data into a **higher dimension** where a linear boundary can separate it.\n",
    "  * Example kernels:\n",
    "\n",
    "    * **Linear** (no transform)\n",
    "    * **Polynomial** (curved boundaries)\n",
    "    * **RBF (Radial Basis Function)** (very flexible, nonlinear boundaries)\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Soft Margin SVM**\n",
    "\n",
    "* Real data often has noise and overlapping classes.\n",
    "* SVM allows some **misclassifications** by introducing a **soft margin**.\n",
    "* A parameter **C** controls the trade-off:\n",
    "\n",
    "  * Large C = less tolerance for errors (tries to classify all correctly, might overfit)\n",
    "  * Small C = more tolerance for errors (more generalized)\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **SVM for Regression (SVR)**\n",
    "\n",
    "* Instead of classification, SVR predicts continuous values.\n",
    "* It tries to fit a function within a margin of tolerance (epsilon) around the data points.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. **Summary of Steps in SVM**\n",
    "\n",
    "| Step                             | Description                                                       |\n",
    "| -------------------------------- | ----------------------------------------------------------------- |\n",
    "| 1. Choose kernel                 | Linear for simple, RBF or Polynomial for complex data             |\n",
    "| 2. Find hyperplane               | Maximize margin between classes                                   |\n",
    "| 3. Use soft margin (parameter C) | Allow some misclassification for robustness                       |\n",
    "| 4. Make predictions              | New points classified based on which side of hyperplane they fall |\n",
    "\n",
    "---\n",
    "\n",
    "## 9. **Example Intuition**\n",
    "\n",
    "Imagine you want to separate apples and oranges on a table:\n",
    "\n",
    "* **Linear SVM:** Draw the straight line that separates apples on left and oranges on right with the largest space between closest apple and orange.\n",
    "* **Non-linear SVM:** If apples and oranges are mixed in a circle, transform the space so you can separate them with a straight line in that transformed space.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. **Python Code Example (Classification)**\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Sample data points\n",
    "X = [[1,2], [2,3], [3,3], [5,5], [6,5], [7,7]]\n",
    "y = [0, 0, 0, 1, 1, 1]  # Classes 0 and 1\n",
    "\n",
    "# Create SVM with RBF kernel (non-linear)\n",
    "model = SVC(kernel='rbf', C=1.0)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict new point\n",
    "print(model.predict([[4,4]]))  # Output: 0 or 1 depending on prediction\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 11. **When to Use SVM?**\n",
    "\n",
    "* When your data has **clear margin** between classes.\n",
    "* Works well in **high dimensional spaces**.\n",
    "* When you want a **robust classifier** with good generalization.\n",
    "* When you have **small to medium-sized datasets** (SVM can be slow on very large datasets).\n",
    "\n",
    "---\n",
    "\n",
    "## 12. **Pros and Cons**\n",
    "\n",
    "| Pros                                         | Cons                                   |\n",
    "| -------------------------------------------- | -------------------------------------- |\n",
    "| Effective in high-dimensional spaces         | Computationally expensive for big data |\n",
    "| Works well with clear margin of separation   | Choosing kernel and parameters tricky  |\n",
    "| Can model non-linear boundaries with kernels | No direct probability outputs          |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84669b93",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
