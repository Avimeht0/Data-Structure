{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b874d471",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üß† Ensemble Learning ‚Äì Quick Notes\n",
    "\n",
    "### üîç Definition:\n",
    "\n",
    "Combining **multiple models** to improve prediction accuracy compared to a single model.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Why Use It?\n",
    "\n",
    "* Reduces **errors**\n",
    "* Improves **accuracy**\n",
    "* Handles **overfitting/underfitting**\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Main Types:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating)**\n",
    "\n",
    "   * Trains models **independently** on random subsets\n",
    "   * Example: **Random Forest**\n",
    "\n",
    "2. **Boosting**\n",
    "\n",
    "   * Trains models **sequentially**, each one corrects the previous\n",
    "   * Example: **AdaBoost, XGBoost**\n",
    "\n",
    "3. **Voting**\n",
    "\n",
    "   * Combines different models' predictions by **majority vote** or **average**\n",
    "   * Example: **Voting Classifier**\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ How it Works:\n",
    "\n",
    "* Same data ‚Üí Multiple models ‚Üí Combine results\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Real-life Examples:\n",
    "\n",
    "* Spam detection\n",
    "* Credit scoring\n",
    "* Stock price prediction\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Key Point:\n",
    "\n",
    "> Ensemble = **Team of models working together** for better results\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33501dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: All libraries imported successfully.\n",
      "Step 2: Loaded the Iris dataset.\n",
      "Features (X) shape: (150, 4)\n",
      "Target (y) shape: (150,)\n",
      "First 5 rows of features (X):\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "First 5 rows of target (y):\n",
      " [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 1. Import libraries\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Step 1: All libraries imported successfully.\")\n",
    "# 2. Load the Iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "print(\"Step 2: Loaded the Iris dataset.\")\n",
    "print(\"Features (X) shape:\", X.shape)\n",
    "print(\"Target (y) shape:\", y.shape)\n",
    "#print data set values \n",
    "print(\"First 5 rows of features (X):\\n\", X[:5])\n",
    "print(\"First 5 rows of target (y):\\n\", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64c27a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Data split into training and testing sets.\n",
      "Training set size: 105\n",
      "Testing set size: 45\n"
     ]
    }
   ],
   "source": [
    "# 3. Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Step 3: Data split into training and testing sets.\")\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Testing set size:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abfb1069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Defined 3 base models:\n",
      "- Logistic Regression\n",
      "- Decision Tree\n",
      "- Support Vector Machine (SVM)\n"
     ]
    }
   ],
   "source": [
    "# 4. Define base models\n",
    "model1 = LogisticRegression(max_iter=200)\n",
    "model2 = DecisionTreeClassifier()\n",
    "model3 = SVC(probability=True)  # Important: probability=True needed for soft voting\n",
    "\n",
    "print(\"Step 4: Defined 3 base models:\")\n",
    "print(\"- Logistic Regression\")\n",
    "print(\"- Decision Tree\")\n",
    "print(\"- Support Vector Machine (SVM)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a25d74ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Created VotingClassifier with soft voting.\n"
     ]
    }
   ],
   "source": [
    "# 5. Create the VotingClassifier ensemble\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('lr', model1),   # 'lr' is just a name label\n",
    "    ('dt', model2),\n",
    "    ('svc', model3)\n",
    "], voting='soft')  # Use 'soft' for probability averaging\n",
    "\n",
    "print(\"Step 5: Created VotingClassifier with soft voting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89fa934f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Trained the VotingClassifier on training data.\n",
      "Step 7: Made predictions on the test set.\n",
      "Predicted labels: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0]\n",
      "Step 8: Calculated accuracy of the ensemble model.\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 6. Train the ensemble model\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "print(\"Step 6: Trained the VotingClassifier on training data.\")\n",
    "# 7. Predict using the ensemble model\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "print(\"Step 7: Made predictions on the test set.\")\n",
    "print(\"Predicted labels:\", y_pred)\n",
    "# 8. Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Step 8: Calculated accuracy of the ensemble model.\")\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ecb73",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üî∂ 1. **Max Voting (Majority Voting)**\n",
    "\n",
    "**Use case**: Mostly for **classification** problems.\n",
    "\n",
    "### üß† Idea:\n",
    "\n",
    "Each model votes for a class (like ‚Äúyes‚Äù or ‚Äúno‚Äù), and the class that gets the most votes is the final result.\n",
    "\n",
    "### üßæ Example:\n",
    "\n",
    "Suppose you have 3 models, and they predict:\n",
    "\n",
    "* Model A: **Yes**\n",
    "* Model B: **No**\n",
    "* Model C: **Yes**\n",
    "\n",
    "Now, count the votes:\n",
    "\n",
    "* \"Yes\" ‚Üí 2 votes\n",
    "* \"No\" ‚Üí 1 vote\n",
    "\n",
    "‚úÖ **Final prediction** = **Yes**\n",
    "\n",
    "### ‚ûï Pros:\n",
    "\n",
    "* Simple and intuitive.\n",
    "* Works well if individual models are diverse and moderately accurate.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∂ 2. **Average Voting (Averaging)**\n",
    "\n",
    "**Use case**: Mostly for **regression** problems (predicting numbers), but can be used in classification if using probabilities.\n",
    "\n",
    "### üß† Idea:\n",
    "\n",
    "Take the average of the predictions from all models.\n",
    "\n",
    "### üßæ Example (Regression):\n",
    "\n",
    "Model predictions:\n",
    "\n",
    "* Model A: 4.2\n",
    "* Model B: 5.0\n",
    "* Model C: 4.8\n",
    "\n",
    "Average = (4.2 + 5.0 + 4.8) / 3 = **4.67**\n",
    "\n",
    "‚úÖ **Final prediction** = **4.67**\n",
    "\n",
    "### üßæ Example (Classification with probabilities):\n",
    "\n",
    "Class \"Yes\" probabilities:\n",
    "\n",
    "* Model A: 0.60\n",
    "* Model B: 0.80\n",
    "* Model C: 0.70\n",
    "\n",
    "Average probability for ‚ÄúYes‚Äù = (0.60 + 0.80 + 0.70)/3 = **0.70**\n",
    "\n",
    "‚úÖ Final class = ‚ÄúYes‚Äù (if we use a threshold like 0.5)\n",
    "\n",
    "### ‚ûï Pros:\n",
    "\n",
    "* Smooths out extreme predictions.\n",
    "* Works well when models are reasonably calibrated.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∂ 3. **Weighted Voting (or Weighted Averaging)**\n",
    "\n",
    "**Use case**: Both **classification and regression**.\n",
    "\n",
    "### üß† Idea:\n",
    "\n",
    "Same as max or average voting‚Äîbut models are not treated equally. Models that perform better get more **weight**.\n",
    "\n",
    "### üßæ Example (Weighted Average - Regression):\n",
    "\n",
    "Model predictions:\n",
    "\n",
    "* Model A: 4.2 (weight = 0.2)\n",
    "* Model B: 5.0 (weight = 0.5)\n",
    "* Model C: 4.8 (weight = 0.3)\n",
    "\n",
    "Final prediction:\n",
    "\\= (4.2√ó0.2 + 5.0√ó0.5 + 4.8√ó0.3)\n",
    "\\= (0.84 + 2.5 + 1.44) = **4.78**\n",
    "\n",
    "‚úÖ Final prediction = **4.78**\n",
    "\n",
    "### üßæ Example (Weighted Majority Voting - Classification):\n",
    "\n",
    "Class \"Yes\" probabilities and weights:\n",
    "\n",
    "* Model A: 0.60 (weight = 0.2)\n",
    "* Model B: 0.80 (weight = 0.5)\n",
    "* Model C: 0.70 (weight = 0.3)\n",
    "\n",
    "Weighted probability:\n",
    "\\= (0.60√ó0.2 + 0.80√ó0.5 + 0.70√ó0.3) = 0.12 + 0.4 + 0.21 = **0.73**\n",
    "\n",
    "‚úÖ Final class = \"Yes\"\n",
    "\n",
    "### ‚ûï Pros:\n",
    "\n",
    "* Gives more power to better models.\n",
    "* More flexible and often more accurate than equal-weight methods.\n",
    "\n",
    "---\n",
    "\n",
    "## üîö Summary\n",
    "\n",
    "| Method              | Use case                   | Idea                                   |\n",
    "| ------------------- | -------------------------- | -------------------------------------- |\n",
    "| **Max Voting**      | Classification             | Pick the class that most models choose |\n",
    "| **Average Voting**  | Regression / Probabilities | Take the average of all predictions    |\n",
    "| **Weighted Voting** | Both                       | Give more weight to better models      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57dd17b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Iris Dataset (First 5 Rows):\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "\n",
      "### Shape of Train and Test Data:\n",
      "Training set shape: (105, 4), Test set shape: (45, 4)\n",
      "\n",
      "### Models Initialized:\n",
      "1. Decision Tree Classifier\n",
      "2. Support Vector Machine (SVC)\n",
      "3. k-Nearest Neighbors (KNN)\n",
      "\n",
      "### Models Trained on Training Data\n",
      "\n",
      "### Predictions from each model on Test Data:\n",
      "Model 1 (Decision Tree) Predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0]\n",
      "Model 2 (SVC) Predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0]\n",
      "Model 3 (KNN) Predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0]\n",
      "\n",
      "### Applying Max Voting (Majority Voting):\n",
      "Max Voting Final Predictions: [np.int64(1), np.int64(0), np.int64(2), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(0), np.int64(2), np.int64(0), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(2), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(1), np.int64(1), np.int64(0), np.int64(0)]\n",
      "\n",
      "### Applying Average Voting:\n",
      "Average Voting Final Predictions: [1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0, 0]\n",
      "\n",
      "### Applying Weighted Voting:\n",
      "Weighted Voting Final Predictions: [1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0, 0]\n",
      "\n",
      "### Evaluating the Accuracy of Each Voting Method:\n",
      "Accuracy of Max Voting: 1.0\n",
      "Accuracy of Average Voting: 1.0\n",
      "Accuracy of Weighted Voting: 1.0\n",
      "\n",
      "### Summary of Results:\n",
      "Max Voting uses majority class voting for each sample.\n",
      "Average Voting calculates the average of predictions and rounds to the nearest class.\n",
      "Weighted Voting assigns a higher weight to the more reliable models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Load and Inspect the Data\n",
    "# -----------------------------\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "\n",
    "# Convert to DataFrame for easy inspection\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "print(\"### Iris Dataset (First 5 Rows):\")\n",
    "print(df.head())  # Prints the first 5 rows of the dataset\n",
    "\n",
    "# ------------------------------\n",
    "# Step 2: Prepare the Data\n",
    "# ------------------------------\n",
    "\n",
    "X = data.data  # Feature matrix\n",
    "y = data.target  # Target vector (labels)\n",
    "\n",
    "# Split data into training and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"\\n### Shape of Train and Test Data:\")\n",
    "print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 3: Initialize Models\n",
    "# ------------------------------\n",
    "\n",
    "# Initialize three different models\n",
    "model1 = DecisionTreeClassifier(random_state=42)\n",
    "model2 = SVC(probability=True, random_state=42)\n",
    "model3 = KNeighborsClassifier()\n",
    "\n",
    "print(\"\\n### Models Initialized:\")\n",
    "print(\"1. Decision Tree Classifier\")\n",
    "print(\"2. Support Vector Machine (SVC)\")\n",
    "print(\"3. k-Nearest Neighbors (KNN)\")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 4: Train Models\n",
    "# ------------------------------\n",
    "\n",
    "# Train the models on the training data\n",
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n### Models Trained on Training Data\")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 5: Make Predictions\n",
    "# ------------------------------\n",
    "\n",
    "# Predict with each model on the test data\n",
    "pred1 = model1.predict(X_test)\n",
    "pred2 = model2.predict(X_test)\n",
    "pred3 = model3.predict(X_test)\n",
    "\n",
    "print(\"\\n### Predictions from each model on Test Data:\")\n",
    "print(\"Model 1 (Decision Tree) Predictions:\", pred1)\n",
    "print(\"Model 2 (SVC) Predictions:\", pred2)\n",
    "print(\"Model 3 (KNN) Predictions:\", pred3)\n",
    "\n",
    "# ------------------------------\n",
    "# Step 6: Max Voting (Majority Voting)\n",
    "# ------------------------------\n",
    "\n",
    "print(\"\\n### Applying Max Voting (Majority Voting):\")\n",
    "\n",
    "max_voting_preds = []\n",
    "for p1, p2, p3 in zip(pred1, pred2, pred3):\n",
    "    votes = [p1, p2, p3]\n",
    "    vote_counts = Counter(votes)\n",
    "    final_pred = vote_counts.most_common(1)[0][0]\n",
    "    max_voting_preds.append(final_pred)\n",
    "\n",
    "print(\"Max Voting Final Predictions:\", max_voting_preds)\n",
    "\n",
    "# ------------------------------\n",
    "# Step 7: Average Voting\n",
    "# ------------------------------\n",
    "\n",
    "print(\"\\n### Applying Average Voting:\")\n",
    "\n",
    "avg_voting_preds = []\n",
    "for p1, p2, p3 in zip(pred1, pred2, pred3):\n",
    "    avg_pred = np.mean([p1, p2, p3])\n",
    "    avg_voting_preds.append(round(avg_pred))  # Round to nearest class (0, 1, or 2)\n",
    "\n",
    "print(\"Average Voting Final Predictions:\", avg_voting_preds)\n",
    "\n",
    "# ------------------------------\n",
    "# Step 8: Weighted Voting\n",
    "# ------------------------------\n",
    "\n",
    "print(\"\\n### Applying Weighted Voting:\")\n",
    "\n",
    "# Define weights for each model (Model 1 has the highest weight)\n",
    "weights = [0.5, 0.3, 0.2]\n",
    "\n",
    "weighted_preds = []\n",
    "for p1, p2, p3 in zip(pred1, pred2, pred3):\n",
    "    weighted_pred = (p1 * weights[0] + p2 * weights[1] + p3 * weights[2])\n",
    "    weighted_preds.append(round(weighted_pred))\n",
    "\n",
    "print(\"Weighted Voting Final Predictions:\", weighted_preds)\n",
    "\n",
    "# ------------------------------\n",
    "# Step 9: Evaluate Model Performance\n",
    "# ------------------------------\n",
    "\n",
    "print(\"\\n### Evaluating the Accuracy of Each Voting Method:\")\n",
    "\n",
    "# Calculate accuracy for each voting method\n",
    "accuracy_max = accuracy_score(y_test, max_voting_preds)\n",
    "accuracy_avg = accuracy_score(y_test, avg_voting_preds)\n",
    "accuracy_weighted = accuracy_score(y_test, weighted_preds)\n",
    "\n",
    "print(f\"Accuracy of Max Voting: {accuracy_max}\")\n",
    "print(f\"Accuracy of Average Voting: {accuracy_avg}\")\n",
    "print(f\"Accuracy of Weighted Voting: {accuracy_weighted}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Final Notes\n",
    "# ------------------------------\n",
    "print(\"\\n### Summary of Results:\")\n",
    "print(\"Max Voting uses majority class voting for each sample.\")\n",
    "print(\"Average Voting calculates the average of predictions and rounds to the nearest class.\")\n",
    "print(\"Weighted Voting assigns a higher weight to the more reliable models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23445e0c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üßæ Bagging (Bootstrap Aggregating) ‚Äì Complete Interview Notes\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. Definition\n",
    "\n",
    "**Bagging (Bootstrap Aggregating)** is an ensemble machine learning technique used to improve the **accuracy** and **stability** of models by combining predictions from **multiple learners** trained on random subsets of the training data.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. Purpose\n",
    "\n",
    "* Reduce **variance** of a model (especially decision trees).\n",
    "* Prevent **overfitting**.\n",
    "* Improve **generalization** on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 3. Intuition (Simple Explanation)\n",
    "\n",
    "Instead of training one model on the whole dataset:\n",
    "\n",
    "* Train several models on **different random samples** (with replacement).\n",
    "* Let them **vote** (for classification) or **average** (for regression).\n",
    "* Final prediction = combined result of all models.\n",
    "\n",
    "üéØ Think of asking multiple people the same question and taking the **majority vote** ‚Äì more reliable than trusting just one person.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 4. How Bagging Works (Step-by-Step)\n",
    "\n",
    "1. **Bootstrap Sampling**:\n",
    "\n",
    "   * Randomly create `n` new training sets from the original dataset.\n",
    "   * Sampling is done **with replacement**, so some data points may repeat, others may be left out.\n",
    "\n",
    "2. **Model Training**:\n",
    "\n",
    "   * Train a **separate base learner** (e.g., decision tree) on each bootstrapped dataset.\n",
    "\n",
    "3. **Prediction**:\n",
    "\n",
    "   * **Classification**: Use **majority voting** from all models.\n",
    "   * **Regression**: Take the **average** of predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 5. Diagram (Conceptual View)\n",
    "\n",
    "```\n",
    "Original Data\n",
    "     ‚Üì\n",
    "Bootstrap Samples (with replacement)\n",
    "     ‚Üì          ‚Üì          ‚Üì\n",
    " Model 1    Model 2    Model 3   ... Model n\n",
    "     ‚Üì          ‚Üì          ‚Üì\n",
    "   Prediction1 Prediction2 Prediction3 ...\n",
    "     ‚Üì\n",
    "Final Output (Majority Vote / Average)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 6. Example in Python (Classification)\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Bagging with Decision Tree\n",
    "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 7. When to Use Bagging\n",
    "\n",
    "* When a **model overfits** the training data (high variance).\n",
    "* When working with **unstable models** (e.g., decision trees).\n",
    "* When you want **better performance** through model averaging.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 8. Common Base Learners\n",
    "\n",
    "* **Decision Tree** ‚Äì most common (used in Random Forest)\n",
    "* KNN\n",
    "* Naive Bayes (less common due to low variance)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 9. Real-Life Example: Random Forest\n",
    "\n",
    "* **Random Forest** = Bagging + Random feature selection.\n",
    "* It‚Äôs a collection of decision trees trained using bagging + a twist: at each split, it chooses a **random subset of features**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 10. Pros and Cons\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Reduces **variance** ‚Üí better generalization.\n",
    "* **Simple** to implement.\n",
    "* Can run models **in parallel**.\n",
    "* Improves **accuracy** without increasing bias.\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Doesn't reduce **bias** (only variance).\n",
    "* Less interpretable (many models).\n",
    "* Computationally more expensive.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 11. Bagging vs Boosting (Comparison)\n",
    "\n",
    "| Feature        | Bagging                              | Boosting                             |\n",
    "| -------------- | ------------------------------------ | ------------------------------------ |\n",
    "| Goal           | Reduce variance                      | Reduce bias (and variance)           |\n",
    "| Sampling       | Random sampling **with replacement** | Sequential training with reweighting |\n",
    "| Model Training | In parallel                          | Sequential (dependent on previous)   |\n",
    "| Overfitting    | Less prone to overfitting            | More prone (if not regularized)      |\n",
    "| Common Example | Random Forest                        | AdaBoost, Gradient Boosting          |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 12. Summary (Interview Quick Talk)\n",
    "\n",
    "* **Bagging** = **Bootstrap + Aggregation**.\n",
    "* Train models on **random subsets** of data and **combine** predictions.\n",
    "* Best used with **high-variance models** like decision trees.\n",
    "* Popular example: **Random Forest**.\n",
    "* Main goal: **reduce variance**, **improve stability**, **avoid overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like this as a **PDF**, **flashcards**, or with **diagrams** to practice visually!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b84458",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
